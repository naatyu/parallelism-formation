# Chargement des donn√©es

Une des premi√®res √©tapes de notre entra√Ænement c'est le chargement des donn√©es. C'est donc √©galement un des premiers bottlenecks possible dans notre entra√Ænement. On est √©galement vite confront√© √† un mur lorsque l'on veut manipuler des datasets massifs (plusieurs TB de donn√©es).
Avoir un chargement des donn√©es efficace et optimis√© est donc une premi√®re √©tape √©ssentielle.

# 1 - Map Dataset

Les deux principaux √©l√©ments de PyTorch pour le chargement des donn√©es sont le `Dataset` (disponible sous 2 versions mais on vera cela par la suite...) et le `Dataloader`.
La classe `Dataset` est assez simple:
```py
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]
```
On acc√®de aux donn√©es par un index, ce type de dataset est appel√© 'Map Dataset'. Dans ce type de datasets les donn√©es sont g√©n√©ralement en RAM (ce qui n√©c√©ssite que le dataset passe en RAM...). Il est basique mais assez efficace pour des petits/moyens datasets.

# 2 - Dataloader

Le `Dataloader` est lui beaucoup plus int√©r√©ssant, nottament √† cause des nombreux arguments incompris qu'il poss√®de:
```py
torch.utils.data.DataLoader(
    dataset,
    batch_size=1,
    shuffle=None,
    sampler=None,
    batch_sampler=None,
    num_workers=0,
    collate_fn=None,
    pin_memory=False,
    drop_last=False,
    timeout=0,
    worker_init_fn=None,
    multiprocessing_context=None,
    generator=None,
    *,
    prefetch_factor=None,
    persistent_workers=False,
    in_order=True)
```
Certains valent le coup qu'on s'y penche dessus.

## 2.1 - Batch size

C'est un argument assez simple, il correspond au nombre de samples du dataset que le dataloader doit r√©cup√©rer. Pourtant il peut avoir un impact assez fort d√ª √† l'architecture actuelle des GPUs.

<img src="assets/1_data_loading/warp.png" alt="gpu warp" width="300"/>

Un GPU poss√®de des *warps*. Un warp est compos√© d'un certains nombre de threads (32 pour les architectures r√©centes) et tous les threads d'un warp sont ex√©cut√©s en m√™me temps.\
Ce qui veut dire que si on a un kernel (fonction qui tourne sur un GPU) qui √† besoin que d'un seul thread, tous les autres threads (31 autres) seront en stand by et inutilisable tant que ce thread n'aura pas finit son ex√©cution.\
C'est pour cela qu'on favorise une batch size multiple de 32 car cela correspond au nombre de threads dans un warp. On peut utiliser des multiples de 16 ou 8 mais il vaut mieux √©viter d'aller au del√†, cela peut r√©duire les performances.

## 2.2 - Workers

Un worker est un processus qui va s'occuper du chargement des donn√©es. L'avantage c'est que on peux donc avoir plusieurs workers qui travaillent en m√™me temps pour charger la donn√©e.\
Si on a `num_wokers=0` alors le processus principal (celui qui s'occupe √©galement de tout le reste dans notre entra√Ænement) va s'occuper de charger les donn√©es. En revanche pour `num_workers=N` on va avoir $N$ diff√©rents processus qui vont s'occuper de charger les donn√©es, le processus principal reste focaliser sur le reste de notre entra√Ænement.

<img src="assets/1_data_loading/workers.png" alt="gpu warp" width="400"/>

En revanche ce n'est pas parfait:
- Comme la m√©moire est partag√© entre les processus, cela peut cr√©er des acc√®s concurentiels sur nos donn√©es et donc un potentiel bottleneck.
- Ajouter des workers revient aussi √† augmenter la RAM n√©c√©ssaire ainsi que les op√©rations de communication (les processus utilise l'Inter Process Communication).
- Avoir trop de workers n'est √©galement pas une bonne chose car cela entraine une forte utilisation CPU et donc un effet que l'on appelle le [*Noisy Neighbor*](https://facebookresearch.github.io/spdl/latest/optimization_guide/noisy_neighbour.html). Lorsque l'utilisation CPU est trop forte (√† partir de 75% d'utilisation moyenne entre tous les coeurs), les coeurs CPU sont trop occup√©s √† g√©rer la donn√©e et donc ont moins de temps pour lancer des kernels. Cela va donc ralentir notre entra√Ænement m√™me si on a beaucoup de workers qui chargent nos donn√©es.

Une bonne valeur se trouve en 2 et 6 workers de mani√®re g√©n√©rale. Un benchmark rapide, quelques it√©rations sur votre boucle d'entra√Æment, peut lever le doute.

## 2.3 - Memory Pinning

> Avant de continuer la lecture, cette section aborde des m√©chanismes sur la m√©moire d'un ordinateur. Allez lire l'annexe [RAM et M√©moire virtuelle](#annexe---ram-et-m√©moire-virtuelle) si vous n'√™tes pas familier avec son fonctionnement.

Le memory pinning ou √©pinglage m√©moire (pas tr√®s beau...) est l'un des arguments les plus utlis√© dans le dataloader sans vraiment savoir ce qu'il fait.\
Lorsque l'OS peut mettre une donn√©e (qui est stock√© dans une page) √† la fois dans le disque ou dans la RAM on dit qu'elle est paginable (ce qui est le comportement par d√©faut). Cependant pour transf√©rer cette donn√©e vers le GPU, il est n√©c√©ssaire qu'elle soit dans la RAM, or l'OS peut transf√©rer des pages de m√©moire √† sa guise vers le disque pour lib√©rer de la place pour d'autres processus. On va donc figer (lock) cette donn√©e dans la RAM de fa√ßon a ce qu'elle ne puisse pas se retrouver dans le disque.\
C'est donc ce que va faire l'argument `pin_memory=True`, il va bloqu√© la donn√©e dans la RAM de fa√ßon √† ce qu'elle puisse √™tre transf√©rer vers le GPU. Lorsque cette donn√©e est bloqu√© le GPU peut utiliser une technologie appel√© Direct Acess Memory (DMA) qui permet au GPU de lire and √©crire sans passer pas le CPU.\
L'int√©r√™t c'est que des que la dataloader aura un batch de pr√™t, il sera √©pingl√© √† la RAM. Donc d√®s que le GPU en aura besoin, son transfert sera plus rapide. PyTorch s'occupe de lancer en fond un thread qui fera le bloquage en RAM pendant que l'on ex√©cute d'autres actions dans notre code.\
Il est √©galement int√©ressant de combiner cet argument avec `non_blocking`:
```py
batch.to(device, non_blocking=True)
```
Lorsque l'on active `non_blocking`, PyTorch lance un cuda stream (un flow d'√©xacution d'op√©ration sur le GPU) qui va s'occuper en parall√®le de vers le transfert CPU&rarr;GPU. On a donc un thread d√©di√© qui bloque la donn√©e en RAM et un stream d√©di√© qui fait le transfert vers le GPU, ce qui permet de faire d'autres op√©rations li√© √† notre entrainement (`optimizer.zero_grad` par exemple).

## 2.4 - Prefetching

L'argument `prefetch_factor` permet de cr√©er un buffer de batch ou le dataloader va charger des batch en avance. Si `prefetch_factor=2` alors chaque worker va charger 2 batch en avance. D√®s qu'une it√©ration sera finie et donc que 1 batch aura √©t√© consomm√©, un worker sera attitr√© pour ajouter le batch suivant √† notre buffer.\
L'avantage de cet argument est qu'il nous permet d'avoir constamment des batchs d'avance et de ne pas faire attendre le GPU la pr√©paration d'un batch par le CPU.\
Au vu de la plupart des observations, le param√®tre par d√©faut dans le dataloader suffit (2 batchs en avance). Si vous avez un traitement assez lourd √† faire, essayez d'augmenter cette valeur.

## 2.5 - A retenir

- Priviligiez une batch size multiple de 32
- Utilisez entre 2 et 6 workers dans votre dataloader. Faites un benchmark rapide pour trouver le bon sweet spot et surtout √©vitez une haute utilisation CPU pour ne pas avoir d'effet de noisy neighbor
- Activez le memory pinning et faites en sorte que l'ensemble de vos transfert vers le GPU utilise le param√®tre `non_blocking=True`

# 3 - Memory mapped files

Le probl√®me des gros datasets c'est qu'il ne passe g√©n√©ralement pas en RAM. Leur stockage devient vite compliqu√© et surtout avoir un chargement efficace des samples peut s'av√©rer √™tre une prise de t√™te.\
Certains format populaires comme parquet permette de compresser les donn√©es et un offrent un stockage optimal. Cependant leur lecture n'est pas forcement rapide (d√©compression n√©c√©ssaire), ce qui sera amplifi√© par le nombre de GPUs qui ont besoin de donn√©es.\
Un des format le plus adapt√©s pour les gros datasets est le memory mapping (voir [RAM et M√©moire virtuelle](#annexe---ram-et-m√©moire-virtuelle)). Les memory mapped files (d√©sol√© j'ai pas trouv√© de bonne traduction...) sont principalement des fichiers binaires, et chaque page de la m√©moire virtuelle du programme peut √™tre li√© √† une partie du fichier (g√©n√©ralement 4 KB par d√©faut) au lieu de la RAM. Lorsque l'on a besoin d'une partie sp√©cifique de notre fichier, comme pour la m√©moire classique, l'OS va venir faire la traduction page &rarr; donn√©es, la seule diff√©rence √©tant que la donn√©e n'est pas une frame en RAM mais un bloc de notre fichier.\
Cela pr√©sente 2 gros avantages, on a pas besoin de charger le fichier entier mais seulement une page a un instant $t$ ce qui √©vite de faire exploser notre RAM. De plus la lecture de ces fichier est seulement faite √† partir d'op√©rations m√©moire basique que l'OS sait tr√®s bien faire, ce qui rend la lecture tr√®s rapide.\
Ce format n'est cependant pas parfait, comme on doit charger une page enti√®re √† chaque fois, la lecture al√©atoire n'est pas tr√®s efficace. Il faut donc priviligier la lecture s√©quentielle, et √©viter ou pr√©parer en amont le shuffling des donn√©es.

On peut utiliser numpy pour manipuler des memmap (abr√©viation couramment utilis√© pour memory mapped files). Pour la cr√©ation par exemple:
```py
import numpy as np

data = np.arange(12, dtype=np.float32).reshape(3, 4)

# 'w+' allows reading and writing
mmap_file = np.memmap('example.dat', dtype='float32', mode='w+', shape=(3, 4))
mmap_file[:] = data[:]

# Ensure changes are written to disk
mmap_file.flush()
```
Pour la lecture:
```py
read_only_mmap = np.memmap('example.dat', dtype='float32', mode='r', shape=(3, 4))
```
Attention, les dimensions de vos donn√©es ne sont pas sauvegard√©s dans la memmap, il faut les pr√©cis√©es √† la lecture. Une bonne mani√®re de faire est de cr√©er un header avec `struct` et sauvegarder les donn√©es utiles dedans. Ensuite lors de la lecture de votre memmap, vous pouvez lire le header pour savoir comment charger les donn√©es.
```py
import numpy as np
import struct
import os

shape = (3, 4)
dtype = np.float32
data = np.arange(12, dtype=dtype).reshape(shape)

header_format = '2IH' # 2 unsigned int and one unsigned short int
header_size = struct.calcsize(header_format)
dtype_code = np.dtype(dtype).num
header = struct.pack(header_format, shape[0], shape[1], dtype_code)

file_name = 'data_with_header.mmap'
with open(file_name, 'wb') as f:
    f.write(header)
    f.write(data.tobytes())
```
Lors de la lecture:
```py
with open(file_name, 'rb') as f:
    header = f.read(header_size)
    unpacked_header = struct.unpack(header_format, header)

    read_shape = (unpacked_header[0], unpacked_header[1])
    read_dtype_code = unpacked_header[2]
    read_dtype = np.dtype(read_dtype_code)

mmap_array = np.memmap(
    file_name, 
    dtype=read_dtype, 
    mode='r', 
    offset=header_size, # L'offset permet de sauter l'en-t√™te
    shape=read_shape
)
```
On a donc besoin de savoir uniquement la taille du header √† l'avance plut√¥t que toutes les donn√©es li√© √† la memmap comme sa shape etc..

# 4 - Iterable Dataset

On a vu au d√©but que le dataset classique de PyTorch est le ``Map Dataset``. Cependant pour la lecture de gros datasets, ce n'est pas adapt√©. Il necessite g√©n√©ralement que les donn√©es (ou une grande partie) soient en RAM. De plus il lit les √©l√©ments un par un, donc pour des fichiers qui ne sont pas deja en RAM, ca entraine des lectures lentes.\
Pour cela PyTorch propose un autre type de dataset appel√© `Iterable Dataset`. Il est adapt√© a des cas ou le dataset ne passe pas en RAM. Quand les donn√©es sont trop larges pour passer en RAM, on va lire les donn√©es en flux plutot que en un gros bloc en RAM. Cela s'apparente √† du lazy loading ou l'on charge les donn√©es √† la demande.
```py
class IterableCSVDataset(IterableDataset):
    def __init__(self, filename):
        super().__init__()
        self.filename = filename

    def __iter__(self):
        # Open the file and yield each line as a sample
        with open(self.filename, 'r') as f:
            # Yield each line one by one
            for line in f:
                parts = line.strip().split(',')
                data = float(parts[0])
                label = float(parts[1])
                yield torch.tensor(data), torch.tensor(label)
```
Le dataset utilise maintenant la m√©thode `__iter__` contrairement a `__getitem__`. Cela va yield des samples ou bloc de samples de notre dataset. Dans l'exemple, on ouvre le fichier et yield les lignes une √† une (tant que la ligne n'aura pas √©t√© consomm√©, rien se passe). Le process sera r√©p√©t√© jusqu'a la fin du fichier. On a donc besoin de charger qu'une seule ligne de notre fichier au fur et √† mesure au lieu du fichier entier.\
On peut √©galement avoir plusieurs workers qui chargent les donn√©es mais il faut nous m√™me d√©finir la strat√©gie, autrement ils chargeront tous la m√™me donn√©e.
```py
class MultiWorkerIterableFileDataset(IterableDataset):
    def __init__(self, filename):
        super().__init__()
        self.filename = filename

    def __iter__(self):
        # Get worker information from PyTorch
        worker_info = torch.utils.data.get_worker_info()
        
        # If no worker info is available (single process), read the entire file
        if worker_info is None:
            start_line = 0
            end_line = float('inf')
        else:
            # Partition the data for each worker
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            
            # Each worker gets a slice of the file
            with open(self.filename, 'r') as f:
                # Count total lines
                total_lines = sum(1 for line in f)
            
            per_worker = total_lines // num_workers
            start_line = worker_id * per_worker
            end_line = start_line + per_worker
            if worker_id == num_workers - 1:
                end_line = total_lines # Make sure the last worker gets the remaining lines

        # Stream the data for the assigned slice
        with open(self.filename, 'r') as f:
            for i, line in enumerate(f):
                if i >= start_line and i < end_line:
                    yield line.strip()
                if i >= end_line:
                    break
```
Ici on s√©pare nos donn√©es en bloc √©gaux afin que chaque worker ait une portion √©gale. Puis chaque worker s'occupe de charger sa propre section et de fournir un sample.

# 5 - Sequence Packing

Lorsque l'on cr√©e des batch de donn√©es, on ne peut pas toujours avoir des samples de m√™me taille. Par exemple pour du texte, les phrases ne font pas toutes la m√™me taille. Pour cela on utilise principalement du padding. Lorsque un sample de fait pas la taille souhait√©, on ajoute des valeurs afin que celui ci fasse la bonne taille. Pour reprendre l'exemple du texte, on ajoute des tokkens de padding, souvent `<pad>` qui permettent sp√©cifi√© que ce sont des tokkens fictifs, juste pour avoir la taille voulu.\
Le probl√®me est que ces tokkens de padding ne servent √† rien, ils sont juste la pour combler le trou entre les diff√©rentes phrases de notre batch. Cela revient √† utiliser de la puissance de calcul et de la VRAM pour des tokkens fictifs.\
Pour combler cela, on peut utiliser le 'sequence packing'. Le principe est simple, plut√¥t que d'avoir une seule donn√©e par sample dans un batch, on peut concat√©ner les donn√©es jusqu'a ce que notre sample soit plein. Un exemple en image:

<img src="assets/1_data_loading/packing.png" alt="gpu warp" width="400"/>

Pour du texte, cela revient √† concat√©ner plusieurs phrases dans une seule s√©qence de notre batch. On utilise un token `<eos>` pour signifier que l'on √† atteint la fin d'une phrase et que l'on va lire la suivante. En revanche si on utilise un transformer (ce qui est tr√®s courant) on va avoir besoin de faire quelques modifications. De base le masque d'attention est triangulaire et plein, mais cela implique que des tokkens d'une phrase peuvent voir les tokkens d'une autre phrase (ce que l'on ne souhaite pas). Pour cela on peut modifier le masque d'attention:

<img src="assets/1_data_loading/attn_mask.png" alt="gpu warp" width="500"/>

Lorsque l'on atteint un tokken `<eos>`, on reset notre masque d'attention de fa√ßon a ce que les tokkens ne puissent pas ce voir d'une s√©quence √† l'autre. Voici un exemple de code (cr√©dit: [https://huggingface.co/blog/sirluk/llm-sequence-packing](https://huggingface.co/blog/sirluk/llm-sequence-packing)):
```py
def get_attention_mask_for_packed_sequence(x, token_id, eos: bool = True):
    B, T = x.shape
    eos_idx = (x.view(-1) == token_id).nonzero(as_tuple=True)[0] + eos
    eos_idx_expanded = torch.cat([eos_idx, torch.arange(0,B*T+1,T)]).unique().sort()[0]
    normalized_idx = eos_idx_expanded - (eos_idx_expanded // T) * T
    normalized_idx = torch.where(normalized_idx == 0, T, normalized_idx)
    reps = normalized_idx[1:] - normalized_idx[:-1]
    reps = torch.where(reps < 1, normalized_idx[1:], reps)
    repeated_idx = torch.repeat_interleave(normalized_idx[1:], reps).view(B,1,T).expand(-1,T,-1)
    mask_indices = torch.arange(T).view(1,-1,1).expand(B, -1, T)
    mask = torch.ones(T, T, dtype=torch.bool).tril().expand(B, -1, -1)
    mask = mask.masked_fill(mask_indices >= repeated_idx, False)
    return mask
```
Cela permet de cr√©er le masque d'attention en fonction des tokens `<eos>` ou `<bos>` (begining of sentence) de votre batch. En revanche il faut garder en t√™te que c'est une op√©ration qui peut devenir lourde. Pour √©viter un ralentissement, il est conseill√© d'inclure la cr√©ation du masque sur GPU pour √©viter des transferts m√©moire et profiter de la parall√©listation des GPUs. Pour cela vous pouvez ajouter cette fonction dans votre modele et ajouter un argument `packed=True`:
```py
class SimpleModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, eos_token_id):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=1)
        self.eos_token_id = eos_token_id

    def forward(self, x, packed=True):
        x_emb = self.embedding(x)
        
        attention_mask = None
        if packed:
            attention_mask = get_attention_mask_for_packed_sequence(x, self.eos_token_id)

        x_emb = x_emb.transpose(0, 1)
        
        attn_output, attn_weights = self.attention(
            x_emb,
            x_emb,
            x_emb,
            attn_mask=attention_mask
        )
        
        return attn_output.transpose(0, 1)
```
A partir du moment ou le modele sera sur GPU, le packing se fera sur GPU et sera donc tr√®s rapide.\

# 6 - Pr√©-traitement

Il est assez courant de devoir effectuer des pr√©-traitement sur les donn√©es avant pouvoir les utiliser pour notre mod√®le (tokenization, augmentations et autres). Cependant c'est g√©n√©ralement assez couteux en ressources et faire perdre du temps lors de la cr√©ationd des batchs. Certains arguments du data loader permettent de r√©duire cela comme `num_workers` ou encore `prefetch_factor` mais sont g√©n√©ralement pas suffisant.\

Pour cela il vaut mieux faire les traitements en amont, puis sauvegarder le r√©sulat dans un format efficace (comme les memmaps par exemples üòä). Cela permettra de r√©cup√©rer les donn√©es deja pr√™tes lors de l'entrainement pour maximiser au mieux l'utilisation GPU contre plus d'espace de stockage.\
Pour un dataset de texte, on peut imaginer avoir notre dataset original au format ``parquet`` (compress√© donc moins de stockage n√©c√©ssaire) puis faire sur CPU notre tokenization que l'on va sauvegarder dans des memmaps. On pourra ensuite utilser ces memmaps lors de l'entraiment et ne pas avoir besoin de faire de tokenization √† la vol√© ce qui permet de reduire consid√©rablement le temps de g√©n√©rtion d'un batch. Les CPUs seront ausi moins occup√© et donc permet d'√©viter l'effet de noisy neighbor.\
Les inconv√©nients sont que l'on a besoin de plus d'espace disque et que ca peut vite devenir cons√©quent. Cela fige aussi nos donn√©es et donc si on fait des modifications sur les donn√©es/m√©thode de traitement on doit reg√©n√©rer notre dataset.

Dans un cas plus extreme ou l'on a un pre-traitement trop lourd pour √™tre fait √† l'avance, on peut essayer de le d√©porter sur d'autres ressources. Par exemple on peut mettre en place un serveur CPU qui s'occupe du traitement des donn√©es, puis par le r√©seau, le serveur GPU vient r√©cup√©rer les donn√©es trait√©s pour sont entrainement.

# Annexe - RAM et M√©moire virtuelle

La m√©moire d'un ordinateur se compose de 2 principaux √©l√©ments, la m√©moire physique et la m√©moire virtuelle.

## M√©moire physique

Plus commun√©ment appel√© RAM (Random Access Memory), la RAM repr√©sente la m√©moire physique de l'odinateur.\
On peut voir la RAM comme une grande grille de bo√Æte ou chaque bo√Æte poss√®de un identifiant bien pr√©cis, plus connue sous le nom d'adresse. Chaque bo√Æte de cette grille peut contenir des donn√©es. Pour r√©cup√©rer ces donn√©es on a donc besoin de connaitre l'adresse exacte vers la bo√Æte.\
La gestion de la m√©moire physique est complexe. Elle est laiss√© au syst√®me d'exploitation (OS), autrement ce serait le chaos total si chaque programme g√©rait sa m√©moire. Un programme peut √©galement avoir besoin de plus de RAM que n√©c√©ssaire, la m√©moire physique ne suffit donc pas dans ce cas la et il va falloir utiliser d'autres moyens/techniques

## M√©moire virtuelle

Pour faciliter la gestion de la RAM ainsi que le cas ou certains programmes ont besoin de plus de m√©moire que ce qui est disponible physiquement, on utilise la m√©moire virtuelle et c'est l'OS qui s'en occupe.\
Pour chaque programme l'OS va cr√©er une m√©moire virtuelle. Ca se pr√©sente sous la forme d'une table qui est priv√©e au programme, un programme √† acc√®s uniquement √† sa m√©moire virtuelle. Cette table commence √† l'adresse 0.\
Cette table est faite de pages qui font par d√©faut 4 KB. L'OS de son c√¥t√© maintient une seconde table, la table des pages ou page table. C'est gr√¢ce √† cette table que l'OS peut traduire une adresse virtuelle vers une adresse physique ou plut√¥t traduit une page vers une frame (bloc physique de m√©moire de la m√™me taille que la page). Ainsi pour un programme la m√©moire virtuelle apprait contigu et organis√© mais c'est gr√¢ce √† l'OS qui lui sait ou se trouve r√©ellement les frames.\
Il est √©galement possible que les pages soient li√©s a une frame sur le disque plut√¥t que dans la RAM. Cela permet de continuer √† allouer de la m√©moire m√™me lorsque l'on a plus de place en RAM. Ce principe s'apelle le swapping.\
Pour acc√®l√©rer le tout, les ordinateurs poss√®dent un composant appel√© Memory Management Unit (MMU), qui permet de faire les traductions pages &rarr; frames de mani√®re instantan√©.\
Voici un sch√©ma repr√©sentatif de la gestion de la m√©moire dans notre ordinateur:

<img src="assets/1_data_loading/memoire_virtuelle.png" alt="gpu warp" width="250"/>



